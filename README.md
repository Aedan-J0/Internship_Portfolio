I will be developing an end-to-end model that assigns confidence scores to the output of an LLM (large language model.) These confidence scores are a rating of how likely it was that a hallucination occurred and are reported to users each time the LLM outputs an answer to their query.

I will be responsible for data processing/normalization/imputation, assessing models/architectures thatâ€™ll perform accurately/appropriately given the data, training the model, and producing graphs that describe its performance. I will additionally create technical documentation outlining my journey.
